from typing import List, Tuple

import numpy as np


def get_n_splits(
    df,
    n: int,
    instance_number: int,
    solver_number: int,
    random_state=0,
) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    Generate indices to split data into training and test sets.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing solver evaluation data
    n : int
        Number of splits
    instance_number : int
        Number of instances to select
    solver_number : int
        Number of solver configurations to select
    random_state : int, default=0
        Random state for reproducibility

    Returns:
    -------
    splits : list of tuples
        List of (train_idx, test_idx) tuples where each contains numpy arrays
        with indices of training and test sets respectively

    Notes:
    ------
    1. Randomly selects solver_number solvers and instance_number instances from df
    2. Ensures that all evaluations from one solver are either in train or test set
    """
    rng = np.random.default_rng(random_state)
    splits = []

    if solver_number % n != 0:
        raise ValueError(
            f"solver_number ({solver_number}) must be divisible by n ({n})"
        )

    all_solver_ids = df["solver_id"].unique()
    all_instance_ids = df["instance_id"].unique()

    if len(all_solver_ids) < solver_number:
        raise ValueError(
            f"Not enough solvers in df ({len(all_solver_ids)}) to select {solver_number}"
        )

    if len(all_instance_ids) < instance_number:
        raise ValueError(
            f"Not enough instances in df ({len(all_instance_ids)}) to select {instance_number}"
        )

    selected_solver_ids = rng.choice(all_solver_ids, size=solver_number, replace=False)
    selected_instance_ids = rng.choice(
        all_instance_ids, size=instance_number, replace=False
    )

    subset_df = df[
        df["solver_id"].isin(selected_solver_ids)
        & df["instance_id"].isin(selected_instance_ids)
    ]

    expected_rows = solver_number * instance_number
    if len(subset_df) != expected_rows:
        raise ValueError(
            f"Incomplete data: Found {len(subset_df)} rows instead of expected {expected_rows}"
        )

    solvers_per_fold = solver_number // n
    shuffled_solver_ids = rng.permutation(selected_solver_ids)

    for i in range(n):
        test_solvers = shuffled_solver_ids[
            i * solvers_per_fold : (i + 1) * solvers_per_fold
        ]

        test_idx = subset_df[subset_df["solver_id"].isin(test_solvers)].index.values
        train_idx = subset_df[~subset_df["solver_id"].isin(test_solvers)].index.values

        total_samples = len(train_idx) + len(test_idx)
        if total_samples != expected_rows:
            raise ValueError(
                f"Total samples ({total_samples}) doesn't match expected number ({expected_rows})"
            )

        splits.append((train_idx, test_idx))

    return splits


def permutate_df_by_cost_decreasing(
    df,
    lognormal_mean=0,
    lognormal_sigma=1,
    random_state=0,
):
    """
    Permutate a DataFrame of solver results by cost in decreasing order and generate cutoff times.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing solver evaluation data with columns 'solver_id', 'instance_id', and 'cost'.
    lognormal_mean : float, default=0
        Mean parameter for the lognormal distribution used to generate noise.
    lognormal_sigma : float, default=1
        Standard deviation parameter for the lognormal distribution used to generate noise.
    random_state : int, default=0
        Random state for reproducibility.

    Returns:
    --------
    df : pandas.DataFrame
        Sorted DataFrame with solvers ordered by decreasing average cost.
    cut_off : numpy.ndarray
        Generated cutoff times for each solver-instance pair, calculated as the cumulative
        minimum cost plus lognormal noise, clipped between 0.01 and 300.

    Notes:
    ------
    1. Solvers are ordered probabilistically, with higher average costs having higher
       probability of being selected first.
    2. The function sorts the DataFrame by the new solver order and instance ID.
    3. Cutoff times are generated by adding lognormal noise to the cumulative minimum
       cost for each instance.
    """
    df = df.copy()
    rng = np.random.default_rng(random_state)

    solver_avg_costs = df.groupby("solver_id")["cost"].mean().reset_index()
    cost = solver_avg_costs["cost"].to_numpy()
    probs = cost / np.sum(cost)

    unique_solvers = solver_avg_costs["solver_id"].to_numpy()
    num_solvers = len(unique_solvers)
    solver_permutation = np.zeros(num_solvers, dtype=object)
    remaining_indices = np.arange(num_solvers)
    for i in range(num_solvers):
        curr_probs = probs[remaining_indices] / np.sum(probs[remaining_indices])
        chosen_idx = rng.choice(len(remaining_indices), p=curr_probs)
        solver_idx = remaining_indices[chosen_idx]
        solver_permutation[i] = unique_solvers[solver_idx]
        remaining_indices = np.delete(remaining_indices, chosen_idx)

    solver_order = {
        solver_id: order for order, solver_id in enumerate(solver_permutation)
    }
    df["solver_order"] = df["solver_id"].map(solver_order)
    df = df.sort_values(["solver_order", "instance_id"])
    df = df.drop(columns=["solver_order"])
    cum_min_cost = df.groupby("instance_id")["cost"].cummin().to_numpy()
    noise = rng.lognormal(lognormal_mean, lognormal_sigma, size=cum_min_cost.shape[0])
    cut_off = cum_min_cost + noise
    cut_off = np.clip(cut_off, 0.01, 300)
    return df, cut_off
